{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY/6lxv/10sr+ZLhJZe8ZE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "binHBx-0rhSe",
        "outputId": "de33cdc6-ecb5-48b1-d620-5342383cf832"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "my6-paNyrM6V"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding('cl100k_base')\n",
        "assert enc.decode(enc.encode('hello there')) == 'hello there'\n",
        "\n",
        "enc = tiktoken.encoding_for_model('gpt-4')"
      ],
      "metadata": {
        "id": "sajqm42srdNH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert enc.decode(enc.encode('hello there')) == 'hello there'"
      ],
      "metadata": {
        "id": "vGT_gcuMr1U8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc.encode('hello there')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKEvOkw-r4wu",
        "outputId": "18afa34c-de5c-46a7-be9d-5ea89732b964"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15339, 1070]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yWj5fBwsA8B",
        "outputId": "e61e1646-d7b1-404e-e8bc-46a2cece41b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-09 13:26:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-10-09 13:26:04 (103 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "C_qZQ59wsEVf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = enc.encode(text)"
      ],
      "metadata": {
        "id": "y5F5a_hSr8He"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWf6j0ausKqL",
        "outputId": "dbbb4017-9395-4a34-a363-b1fee8678eda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "301829"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.n_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3wxJMesLiu",
        "outputId": "c0b7191f-435b-48ba-db4d-4b3f46c3ed8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100277"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc.decode([text_tokens[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mVop3UeasNj4",
        "outputId": "fe286603-3901-48b4-b7da-56cb4a1e2c47"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "5HJ7YbGysVtI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(text_tokens, dtype=torch.long)"
      ],
      "metadata": {
        "id": "9uFUTH9IsRNv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZVcFBu2scf7",
        "outputId": "306b22b6-cd98-4c15-ca59-9e6e866d31b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([301829])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 3e-4"
      ],
      "metadata": {
        "id": "cLv9AkrosiKe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text.split(' ')), len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x_EJ454sj3n",
        "outputId": "ae22d548-c3e4-4b77-8739-d1ab4b8bac32"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(169893, 1115394)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text.split(' '))))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "OlHcfDHEsnuI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DmAVItersu9H",
        "outputId": "cbd3852a-9ae2-4253-9a5c-1e528fc83a71"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWas'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ' '.join([itos[x] for x in l])"
      ],
      "metadata": {
        "id": "_cP4Cus3sv6L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text.split(' ')), dtype=torch.long)"
      ],
      "metadata": {
        "id": "83hdkJ-WtEBj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzdB1MhttJrh",
        "outputId": "eb86a1e2-9e93-4954-e943-e6ea6ace32a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1455,   957, 39874, 29614,  5949, 16628, 18572, 24432, 34050, 34057])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(encode(text.split(' ')[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ic2iVWRvtKj5",
        "outputId": "37ae0c36-e225-4af1-8da5-d0acc6a1cd74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "rb6pSSaytPcD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_decay_matrix(dim, gamma):\n",
        "    d = torch.ones(dim)\n",
        "    d = torch.tril(d) # lower triangular matrix\n",
        "\n",
        "    for index, head in enumerate(d):\n",
        "        g = gamma[index]\n",
        "        for idx, x in enumerate(torch.tril(head)):\n",
        "            for idy, y in enumerate(x):\n",
        "                if idx >= idy:\n",
        "                    head[idx][idy] = g ** (idx-idy)\n",
        "    return d"
      ],
      "metadata": {
        "id": "o3T1-fr9uMAP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q einops\n",
        "import einops\n",
        "from einops import rearrange, reduce, repeat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq0eFj5SncDz",
        "outputId": "2aebed82-e8fc-45e8-e29a-adf9645e784e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChunkwiseRetention(nn.Module):\n",
        "    def __init__(self, chunk_size, num_head, block_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, chunk_size*num_head, bias=False)\n",
        "        self.query = nn.Linear(n_embed, chunk_size*num_head, bias=False)\n",
        "        self.value = nn.Linear(n_embed, chunk_size*num_head, bias=False)\n",
        "        self.gamma = 1.0-2.0**(-5-torch.arange(0, num_head))\n",
        "        self.decay_mask = get_decay_matrix((num_head, block_size, block_size), self.gamma)\n",
        "        self.chunk_decay = self.gamma\n",
        "        self.gn = nn.GroupNorm(1, num_head)\n",
        "        self.num_head = num_head\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def forward(self, x, past_kv):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        k = rearrange(k, ('b t (h c) -> b h t c'), t=T, h=self.num_head, c=self.chunk_size)\n",
        "        q = rearrange(q, ('b t (h c) -> b h t c'), t=T, h=self.num_head, c=self.chunk_size)\n",
        "        v = rearrange(v, ('b t (h c) -> b h t c'), t=T, h=self.num_head, c=self.chunk_size)\n",
        "\n",
        "        retention = q @ k.transpose(-1, -2)\n",
        "        retention = retention * self.decay_mask\n",
        "        inner_retention = retention @ v\n",
        "\n",
        "        past_kv = repeat(past_kv, 'n q v -> B n q v', B=B)\n",
        "        pb, pn, pq, pv = past_kv.shape\n",
        "\n",
        "        padding = torch.zeros(pb, pn, pq, self.chunk_size)\n",
        "        past_kv = past_kv + padding\n",
        "\n",
        "        dm = repeat(self.decay_mask, 'h c d -> B h c d', B=B)\n",
        "        pp = q @ past_kv\n",
        "        cross_retention = pp.transpose(-1, -2) @ dm\n",
        "        cross_retention = cross_retention.transpose(-1, -2)\n",
        "\n",
        "        retention = inner_retention + cross_retention\n",
        "\n",
        "        current_kv = self.gamma.view(self.num_head, 1, 1) * past_kv + (k.transpose(-1, -2) @ v)\n",
        "        output = self.gn(retention.transpose(-1, -2))\n",
        "        output = rearrange(output, 'b c h t -> b t (c h)')\n",
        "        return output, current_kv.mean(dim=0)"
      ],
      "metadata": {
        "id": "erQ9J9fVnfyk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedMultiScaleRetention(nn.Module):\n",
        "    def __init__(self, chunk_size, num_head, block_size):\n",
        "        super().__init__()\n",
        "        self.wg = nn.Linear(n_embed, n_embed, bias=False)\n",
        "        self.act = nn.SiLU()\n",
        "        self.y = ChunkwiseRetention(num_head=n_head, chunk_size=n_embed//n_head, block_size=block_size)\n",
        "        self.wo = nn.Linear(n_embed, n_embed, bias=False)\n",
        "        self.past = torch.zeros(num_head, chunk_size, chunk_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        wgx = self.wg(x)\n",
        "        wgx = self.act(wgx)\n",
        "        y, past = self.y(wgx, self.past)\n",
        "        y = wgx * y\n",
        "        return self.wo(y)"
      ],
      "metadata": {
        "id": "vFYFIikMni4r"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4*n_embed),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4*n_embed, n_embed),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "Mch6ry0unlMZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embed, n_head, block_size):\n",
        "        super().__init__()\n",
        "        self.sa_head = GatedMultiScaleRetention(num_head=n_head, chunk_size=n_embed//n_head, block_size=block_size)\n",
        "        self.ffw = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa_head(self.ln1(x))\n",
        "        x = x + self.ffw(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "1zCBNDSQnnKe"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RetNet(nn.Module):\n",
        "    def __init__(self, block_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head, block_size=block_size) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        token_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            b, s = idx.shape\n",
        "            bk = min(s, block_size)\n",
        "            idx_cond = torch.cat((torch.zeros(b, block_size-bk, dtype=int), idx), dim=1)[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "vObpTd5znpDx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split, batch_size):\n",
        "    data = train_data if split=='train' else val_data\n",
        "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "VTdeq0NOnrwS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "block_size = 32\n",
        "max_iters = 5000\n",
        "eval_iterval = 100\n",
        "lr = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embed = 32\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "EYAWZP3PoI4F"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RetNet(block_size=block_size)\n",
        "xb, yb = get_batch('train', batch_size=batch_size)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "logits, loss = model(xb, yb)"
      ],
      "metadata": {
        "id": "AddYvWEynu1z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size=batch_size)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "1I-vpzjSoPmC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % 100 == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: Train Loss {losses['train']:.4f}, Val Loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train', batch_size=batch_size)\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLU9LWS7n_hz",
        "outputId": "1f56837c-b721-43f8-a386-41b86cdfdf6a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Train Loss 11.0299, Val Loss 11.0290\n",
            "Step 100: Train Loss 9.5518, Val Loss 9.6659\n",
            "Step 200: Train Loss 8.3693, Val Loss 8.6311\n",
            "Step 300: Train Loss 8.1147, Val Loss 8.4785\n",
            "Step 400: Train Loss 8.0443, Val Loss 8.4210\n",
            "Step 500: Train Loss 8.0098, Val Loss 8.4514\n",
            "Step 600: Train Loss 7.9541, Val Loss 8.4678\n",
            "Step 700: Train Loss 7.9416, Val Loss 8.4939\n",
            "Step 800: Train Loss 7.9074, Val Loss 8.5544\n",
            "Step 900: Train Loss 7.8954, Val Loss 8.5651\n",
            "Step 1000: Train Loss 7.8639, Val Loss 8.5639\n",
            "Step 1100: Train Loss 7.8472, Val Loss 8.5571\n",
            "Step 1200: Train Loss 7.8231, Val Loss 8.6143\n",
            "Step 1300: Train Loss 7.8032, Val Loss 8.5881\n",
            "Step 1400: Train Loss 7.7807, Val Loss 8.6421\n",
            "Step 1500: Train Loss 7.7787, Val Loss 8.6343\n",
            "Step 1600: Train Loss 7.7459, Val Loss 8.6390\n",
            "Step 1700: Train Loss 7.7198, Val Loss 8.6767\n",
            "Step 1800: Train Loss 7.7155, Val Loss 8.6750\n",
            "Step 1900: Train Loss 7.6969, Val Loss 8.7068\n",
            "Step 2000: Train Loss 7.6798, Val Loss 8.6861\n",
            "Step 2100: Train Loss 7.6476, Val Loss 8.7364\n",
            "Step 2200: Train Loss 7.6047, Val Loss 8.7347\n",
            "Step 2300: Train Loss 7.5866, Val Loss 8.7367\n",
            "Step 2400: Train Loss 7.5729, Val Loss 8.7695\n",
            "Step 2500: Train Loss 7.5431, Val Loss 8.7765\n",
            "Step 2600: Train Loss 7.5098, Val Loss 8.7771\n",
            "Step 2700: Train Loss 7.5014, Val Loss 8.7823\n",
            "Step 2800: Train Loss 7.4590, Val Loss 8.8155\n",
            "Step 2900: Train Loss 7.4381, Val Loss 8.8100\n",
            "Step 3000: Train Loss 7.4203, Val Loss 8.8182\n",
            "Step 3100: Train Loss 7.4079, Val Loss 8.8074\n",
            "Step 3200: Train Loss 7.3714, Val Loss 8.8309\n",
            "Step 3300: Train Loss 7.3419, Val Loss 8.8368\n",
            "Step 3400: Train Loss 7.3215, Val Loss 8.8587\n",
            "Step 3500: Train Loss 7.2663, Val Loss 8.9177\n",
            "Step 3600: Train Loss 7.2791, Val Loss 8.9210\n",
            "Step 3700: Train Loss 7.2410, Val Loss 8.9748\n",
            "Step 3800: Train Loss 7.2295, Val Loss 8.9552\n",
            "Step 3900: Train Loss 7.1987, Val Loss 8.9448\n",
            "Step 4000: Train Loss 7.1709, Val Loss 8.9928\n",
            "Step 4100: Train Loss 7.1516, Val Loss 9.0063\n",
            "Step 4200: Train Loss 7.1158, Val Loss 9.0603\n",
            "Step 4300: Train Loss 7.0843, Val Loss 9.0719\n",
            "Step 4400: Train Loss 7.0565, Val Loss 9.1092\n",
            "Step 4500: Train Loss 7.0616, Val Loss 9.1450\n",
            "Step 4600: Train Loss 7.0169, Val Loss 9.2045\n",
            "Step 4700: Train Loss 6.9693, Val Loss 9.2136\n",
            "Step 4800: Train Loss 6.9716, Val Loss 9.2536\n",
            "Step 4900: Train Loss 6.9446, Val Loss 9.2334\n",
            "Step 4999: Train Loss 6.9145, Val Loss 9.3161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor([encode('thou art kneel before king'.split(' '))], dtype=torch.long)\n",
        "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAXAf72BoRNT",
        "outputId": "9d36fc10-b774-4a4c-cf9b-5d4c6b1788d4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thou art kneel before king with my dear thou dost be dragon: XI:\n",
            "And prosecute\n",
            "'Gainst buzz if silent: there joy at foretold in consul.\n",
            "\n",
            "MENENIUS:\n",
            "Fie, thought love the pent-up up hateful consort'st O, we Master death,\n",
            "Their out what doth you am for't:\n",
            "All with him to her Was you have been led will hinder ear thou, your own.\n",
            "For of commanded able LEWIS Bolingbroke?\n",
            "Gardener, one bloody for daughter:\n",
            "The hearten 's you to save my part, sweat the rest,\n",
            "Yet ancient boast\n",
            "As the tub.\n",
            "\n",
            "LUCIO:\n",
            "Why, of wretched for thy part, put ELIZABETH:\n",
            "Ah, and there threats!\n",
            "\n",
            "YORK:\n",
            "Will thou hast with thine with he,\n",
            "'That his thing seek You'll him hurt in thy mind; oath?\n",
            "As makes the greater issue\n",
            "Of are than news?\n",
            "\n",
            "Second thou hair\n",
            "Shall, Pray straight;\n",
            "No pardons,\n",
            "That OF AUMERLE:\n",
            "Where grumbling will steal and friend\n",
            "And confess,\n",
            "Great thought this part you take remember done?\n",
            "\n",
            "SICINIUS:\n",
            "The these good made\n",
            "Good master more for water-drops!\n",
            "Good that ever Bolingbroke.\n",
            "\n",
            "SIR good signior, on this taverns hands;\n",
            "Swear yourself I not touch'd skulls; hath one I were no good heartily to get,\n",
            "And Edward society:\n",
            "Now deceit the door him.\n",
            "\n",
            "ISABELLA:\n",
            "Peace footman, more: which, thou power: thee, was songs and to serve we will gone?\n",
            "\n",
            "GLOUCESTER:\n",
            "He nights this night of blessed coward,\n",
            "As presently.' dropp'd shall while.\n",
            "Please what which 'twas so hired with peck thee\n",
            "To pray which was you would will be awhile, up.\n",
            "\n",
            "DUCHESS how to the Henry speak, that prophesied another bless of fates to stage But, thee, being most\n",
            "strange arm,\n",
            "And sons.\n",
            "I'll a tapster, Custom are abhor,\n",
            "And you king?\n",
            "Awake, that succession a dire fair MARGARET:\n",
            "I'll the\n",
            "great me my go.\n",
            "\n",
            "CLAUDIO:\n",
            "One to be sly, resolute.\n",
            "\n",
            "OXFORD:\n",
            "I madam:\n",
            "To of death?\n",
            "\n",
            "Page:\n",
            "My better:\n",
            "By yourself state drybeat money betwixt indeed it is woes with\n",
            "continency; what of the wounds\n",
            "Open this folly, scope my father like\n",
            "mummers; money GREY:\n",
            "Be you\n",
            "bald-pated, have worn am love!\n",
            "When thou, I looks Gloucester's no of this names defy God--God comfortable was that's all good face uncle, if unless he make a young,\n",
            "So than kiss ill may at triumph nor the head of Gloucester, he of nine, thee us about,\n",
            "When of thy sue\n",
            "His envy. am battle, of all, night, love know the grace I know.\n",
            "\n",
            "DUKE of Gloucester, and father, lord and tears should have a friend a innocent fine day.\n",
            "Take her?\n",
            "\n",
            "DUKE first in so butcher? as York Thomas worn change not till so old flower how know what I am not never thine a Aufidius gracious love has son where no sovereign bear for reproof.\n",
            "\n",
            "DUCHESS thee.\n",
            "But to be ill:\n",
            "Her IV:\n",
            "Now, thus knows, am do, you would need he did.\n",
            "\n",
            "YORK:\n",
            "Richard I cannot bow-boy's not God thou art you never life\n",
            "And else call the farewell.\n",
            "\n",
            "EARL are known,\n",
            "There head law;\n",
            "The me wore his course chiefly with your lord's that good.\n",
            "\n",
            "EMILIA:\n",
            "Now cares nine of deceit?\n",
            "And is luck\n",
            "To Isabel, two brother while.\n",
            "You're tears to come; that lords:\n",
            "This us.\n",
            "\n",
            "TITUS:\n",
            "\n",
            "COMINIUS:\n",
            "Noble the elements\n",
            "Of of thee,\n",
            "When high rock, back'd to make you bear\n",
            "I' but made gross honey in grace\n",
            "Equal VI:\n",
            "But, is die.\n",
            "\n",
            "KING is myself.\n",
            "\n",
            "KING murder'd\n",
            "Came of death;\n",
            "Whose your grace in the time senators his queen:\n",
            "Now itch spoons,\n",
            "Irons he allegiance morning to come let would to Buckingham sure how thou ride? to shake prayer\n",
            "To a leg, is in their bird.\n",
            "\n",
            "JULIET:\n",
            "Sweet, tempt from him. in 'pardon' keep'st,\n",
            "Hourly is break awry: dear-a?\n",
            "Any of love\n",
            "And other.\n",
            "A what they have no rapture take the banish'd my that your foe is Bolingbroke.\n",
            "\n",
            "SIR in God's than God when thy most 'shall' stood been cheering they that physicians did; confound.\n",
            "\n",
            "DUCHESS let's out, a world spirit I watch every ELIZABETH:\n",
            "Nay, benefactors possession means, an with child.\n",
            "\n",
            "LUCIO:\n",
            "Believe will words, arrived Bianca to this like yet through love;\n",
            "For with my royal hands.\n",
            "\n",
            "YORK:\n",
            "While thou, enemies' great day\n",
            "As she in love wash he, I be us\n",
            "Our head.\n",
            "They and holy;\n",
            "Not the brat once men\n",
            "Did seal'd are upon my wrongs:\n",
            "That in the trumpets things BOLINGBROKE:\n",
            "Kind their enmity.\n",
            "\n",
            "JULIET:\n",
            "I O bosom.\n",
            "And them, break an shall make hope, of he hath says live\n",
            "on lie of his chains viol he may with pearl,\n",
            "Inestimable I himself.\n",
            "I in vehement know; saint,\n",
            "Were she? offer not; requite is die,\n",
            "With my child!\n",
            "Dead plague love not pay the bastard.\n",
            "Thou me in Clarence best news!\n",
            "\n",
            "DORSET:\n",
            "Be him did Jove's of mine fury swallow say, to reward\n",
            "What prove evil; of the truth; valiant for her\n",
            "As burning KING them royalties know't.\n",
            "\n",
            "First feet forward; should not that damnable.\n",
            "\n",
            "First all no hatred ill the thing let his grace.\n",
            "\n",
            "CATESBY:\n",
            "I'll shall like of Clarence: made you sigh, groom and howled excuses;\n",
            "Nor heavy no solely: bright said thou'rt Now good little was thou mortal own best Luke's: of traitors thy last,\n",
            "Before favours more prosperous: stands with thee,\n",
            "Than rather he actions\n",
            "show honour,\n",
            "To and\n",
            "sought were getter sands of about your beauty:\n",
            "Thou and good despair, the people's Alas,\n",
            "He in tithe-pig's no one at restless prey thee whiles beseech some for too else me, is itself\n",
            "England kill'st but me by base shall you say, o' him!\n",
            "And she but so much wide is them? if wish by garment,\n",
            "not therefore, I, name toward I did Prince, king, is to bloody desire his pre-contract:\n",
            "To mount VI\n",
            "\n",
            "KING and hanging.\n",
            "\n",
            "POMPEY:\n",
            "If alas,\n",
            "Than to give our knees.\n",
            "To one word.\n",
            "\n",
            "KING in them?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Still thy unrest will not have one of duke:\n",
            "good dead!\n",
            "\n",
            "JULIET:\n",
            "Can seal'd and amazed peace home will thus sir?\n",
            "\n",
            "SAMPSON:\n",
            "\n",
            "GREGORY:\n",
            "No.\n",
            "\n",
            "SAMPSON:\n",
            "No, lest salter less, and by her comes the other Will't BOLINGBROKE:\n",
            "O of armies?\n",
            "\n",
            "Messenger:\n",
            "Within the king, is so must my son of a duellist, how ho! hath it then forth. and I is\n",
            "the parch'd wise which, a wife.\n",
            "\n",
            "Second of stooping,--\n",
            "\n",
            "AUFIDIUS:\n",
            "That RICHARD III:\n",
            "What sir; having deadly York ring friends: in wonder the Black to madam! weep is there again; thy ghost,\n",
            "To for virtuous is not come, deserve a is done. fair about the hour fair soul-vex'd,\n",
            "And intitle very 'Whoop, more;\n",
            "For of Lancaster with such despair?\n",
            "\n",
            "BONA:\n",
            "My there;\n",
            "And hadst of credent modern live of me--\n",
            "\n",
            "AUTOLYCUS:\n",
            "O, after majesty\n",
            "To of servant your land.\n",
            "\n",
            "HENRY your sweet thou\n",
            "wicked will than 'ay' you may no; I cannot RICHARD shrewd on possession.\n",
            "3 and need Edward you did else?\n",
            "\n",
            "QUEEN loves I then like Jesu fortune that and my brother,\n",
            "Or prince;\n",
            "For Juno, my sovereign, Mercutio, in you,\n",
            "Let not; maids calls make by the visitation with his son of light a piece my cabin being surely:\n",
            "comes a\n",
            "cat, them, though heaven my teeth to see the rest, royally their city?\n",
            "\n",
            "POMPEY:\n",
            "They Edward to cry where heaven not, Duke thou soul each return\n",
            "for that notorious and York, before Lancaster!\n",
            "\n",
            "GLOUCESTER:\n",
            "Thou so and\n",
            "turns blood\n",
            "Hath and you, here hear but overheard swear virtuous condemn'd for Oxford her,\n",
            "That good Servingman:\n",
            "Who, noble Valiant fly!\n",
            "\n",
            "WARWICK:\n",
            "Why, say scope.\n",
            "\n",
            "CLAUDIO:\n",
            "But him:\n",
            "He to you of you, before thyself,\n",
            "From He are I will\n",
            "presently the fortunes pitch she will you are, to the\n",
            "hedge hath solicit hanging into quickly not.\n",
            "\n",
            "TYBALT:\n",
            "Follow had made them perfect a testimony home\n",
            "with is 's in the parley; for his coming me,\n",
            "For stop vain.\n",
            "\n",
            "DUKE alack Who the author, if flat;\n",
            "To the other therefore and honour.\n",
            "\n",
            "Senators:\n",
            "To and as resembles it O royal golden like offended your son,--believe and mind\n",
            "Transform'd estate him see streams with this?\n",
            "O, doth doing yet though long.\n",
            "\n",
            "HENRY I'll still\n",
            "To power have we not trouble will perish\n",
            "And home:\n",
            "But prancing on others: upon looks,\n",
            "Infusing full of\n",
            "hanging, of the\n",
            "like, thou upon becomes,\n",
            "While by meeting They with your disposition, I do I\n",
            "Persuade saluteth comes I'll as thou habit,\n",
            "Wrench in blood, string, of pearl,\n",
            "Inestimable Capulet; out of mine;\n",
            "It cold letter's pilgrimage.\n",
            "\n",
            "JOHN live:\n",
            "Had and himself two hard\n",
            "When fares:\n",
            "By Overdone I must buy of little joy and BOLINGBROKE:\n",
            "I deed lose thoughts:\n",
            "So a deep.\n",
            "\n",
            "LOVEL:\n",
            "Come, to fast.\n",
            "\n",
            "MERCUTIO:\n",
            "Where being Courtney, shame the doth.\n",
            "\n",
            "KING should have Lucentio,\n",
            "Come to your royal sky.\n",
            "I to been who hence; and the white boast beseech I weep for right\n",
            "Hath What leave,\n",
            "I'll calm, monthly help;\n",
            "Our of Antigonus,\n",
            "Since arms,\n",
            "Let's sleeping free of Edward's sweet sons\n",
            "Shall it even where you should approaches.\n",
            "\n",
            "MARIANA:\n",
            "Will't bear advantage that will tell you have all much in my lord, his cause.\n",
            "\n",
            "Lord you.\n",
            "\n",
            "POLIXENES:\n",
            "You time can where thou darest.\n",
            "\n",
            "DUKE I have thy day sir; not rite?\n",
            "What were seized and by mighty confusions. female no old my lord in us.\n",
            "\n",
            "MENENIUS:\n",
            "Either and he thou here bear you die.\n",
            "\n",
            "ESCALUS:\n",
            "You Vaughan, winters strength\n",
            "Than believe I\n",
            "Do my brains than you must the world to hang with Richard\n",
            "Than you I may to purge thee he was will vouchsafed,\n",
            "With to right.\n",
            "Well, we shall look you shall true: him: Rome, against dear for length\n",
            "Your OF GAUNT:\n",
            "Come, and say, and ELIZABETH:\n",
            "I eyes, when this casque to exile; says.\n",
            "\n",
            "DUKE receive you had so vilely\n",
            "Yielded creation thou honour\n",
            "In like ta'en the blood for I,\n",
            "Drinking them a kind should.\n",
            "Lest of\n",
            "honesty the furthest admire, and prove very time me that noble knee\n",
            "I my heart; that majesty, resolved sworn admire\n",
            "This be grave,\n",
            "Being OF YORK:\n",
            "Bound and, to stand: now;\n",
            "And this devise! and our waters in you life\n",
            "We early this, for that man out to broke deceit see marriage.\n",
            "\n",
            "HASTINGS:\n",
            "Why, be cease.\n",
            "But take it, to my sorrow's King will but day and delicious fair?\n",
            "Farewell: your highness,\n",
            "My we wot too rough, abhor show they have some close, fawns, you, heavy.\n",
            "Ratcliff!\n",
            "\n",
            "RATCLIFF:\n",
            "My you.\n",
            "\n",
            "First power not you rather all\n",
            "From most twenty thee, not, no, my temper; not these dear on eggs they tuft my heart, God at bury looks\n",
            "My is Mowbray's it.\n",
            "\n",
            "QUEEN prove Angelo. breeding\n",
            "That our mettle each did is word was then of an place canon, corrupt a sweet,\n",
            "Fertile sir, bring him loath this Paulina.\n",
            "\n",
            "PAULINA:\n",
            "Will to have it, come,\n",
            "Where Thomas pardon'd and play and gates methinks is all!\n",
            "But, but this jest me not as truce hither, tell thee.\n",
            "Heaven Katharina, of a honour; daughter,\n",
            "And must use my maidenhead!\n",
            "\n",
            "Nurse:\n",
            "Hie 'tis my thought: reposeth a\n",
            "process-server, been not with wife.\n",
            "Familiarly foot.\n",
            "\n",
            "CORIOLANUS:\n",
            "Give never Duke of king? Camillo, thing which was day indeed!\n",
            "\n",
            "LADY RICHARD soul! not Mowbray, BOLINGBROKE:\n",
            "I of Rome the time for life.\n",
            "Apollo, them to high voices I'll to be so. hast ta'en,\n",
            "That this fellow with a fortune that:\n",
            "an sad:\n",
            "You and carelessly days stand kindling should in his sacred the\n",
            "drum yest testimonies in receipt lunatic made you will I wept, for all Christian-like but a taste death it shall all he be of righteous follow'd\n",
            "Should not Lord, emmew\n",
            "As them land say't the contract rather will sir.\n",
            "\n",
            "ESCALUS:\n",
            "Alas, that know'st this God's lord:\n",
            "If, is wounds them.\n",
            "\n",
            "ROMEO:\n",
            "Art to the themselves. our God, I wot.\n",
            "\n",
            "HENRY our fortune gave would tear with this thing hours amen, fast-falling 'tis curs of thine down.\n",
            "\n",
            "MONTAGUE:\n",
            "Alas, let\n",
            "the Richard, are attempt;\n",
            "Therefore had we were shame, if it raged him--ah, lord of music, sanctuary\n",
            "And Rome;\n",
            "But, holy all thee! of triumph! for\n",
            "tying therewithal\n",
            "Remit on the warlike honesty if as a ravisher, their suppliants not. land: with an more sacrificing king, am better of my death and not keep one provoke been a promontory,\n",
            "And not have framed in a warlike wrong trust please I'll together: found me.\n",
            "\n",
            "GLOUCESTER:\n",
            "Sweet gentleman?\n",
            "This shall confess and which, the devil thank one fall. of melt approach:\n",
            "I was a further crown to tear\n",
            "The ready? to make hap? life\n",
            "And words;\n",
            "Whilst with friend late so,\n",
            "And be\n",
            "silent, the power grave's with.\n",
            "Why for frail.\n",
            "\n",
            "STANLEY:\n",
            "So to exclaims.\n",
            "\n",
            "KING with such which Richard adversaries\n",
            "To-morrow am us be it may: the lie.\n",
            "\n",
            "Clown:\n",
            "Your before welcome, disposition,\n",
            "But of world\n",
            "I to-night\n",
            "Have to be mew'd himself!\n",
            "\n",
            "WESTMORELAND:\n",
            "Base, rat-catcher, make you think for rotten Henry thy never friends, 'God lie did; fa my deceased me.\n",
            "\n",
            "CLARENCE:\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "LADY now he were you else dear\n",
            "To and put Warwick than his surplus of his way:\n",
            "So come, late,\n",
            "O'erthrows Will't hot? she sure\n",
            "All commands bid where gave doth doubtful:\n",
            "I thou buy but a colour. now?\n",
            "\n",
            "CHRISTOPHER:\n",
            "At haply art and Margaret:\n",
            "But in this first us him love\n",
            "That bear and is,\n",
            "When raven! queen, LAURENCE:\n",
            "\n",
            "PARIS:\n",
            "Happily adventure men I love him with a weak gown.\n",
            "\n",
            "Fourth then and bait unto your brother's Agamemnon's fills I'll pawn'd;\n",
            "The streets, thou consul.\n",
            "\n",
            "BRUTUS:\n",
            "Then then him bound he'll pitiful, you, about I be with the man, they did slain, to kings!\n",
            "I cannot will severe.\n",
            "\n",
            "ESCALUS:\n",
            "It farewell, pretty lords, hear right and away!\n",
            "Delay if it the gods of this gaze desert!\n",
            "\n",
            "CORIOLANUS:\n",
            "Ay, your love,\n",
            "And pray speak\n",
            "To thee not fiends not oppose Mercury away.\n",
            "What's nobleness are such fear you were for never with you know depart not speak I charge you, while many to wail sigh:\n",
            "Speak as that you tell the world.\n",
            "\n",
            "HORTENSIO:\n",
            "Petruchio, have sad Romeo,\n",
            "If I give me do uncle, ready in you, I lend the imagined may have LEWIS give you have greets to be us.\n",
            "\n",
            "RICHMOND:\n",
            "What crown must be so; leave I cannot be then from the consent on charity;\n",
            "Made affliction out\n",
            "Give his eloquence.\n",
            "Now, hated heart-blood calling lay were your brows, came it me, that he Bolingbroke, Duke of this?\n",
            "\n",
            "VOLUMNIA:\n",
            "Good ground,\n",
            "I that VINCENTIO:\n",
            "My get a piece you did soldiers blood\n",
            "That may but an gentle be\n",
            "remembered, of Edward's corn, Mercutio?\n",
            "Tybalt, I say more in proof, Hastings, slept as my brother sun hence,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKLSKPnUygb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}