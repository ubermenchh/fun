{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU langchain chromadb langchainhub bs4 gradio sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2023-11-27T05:38:21.053413Z","iopub.execute_input":"2023-11-27T05:38:21.053807Z","iopub.status.idle":"2023-11-27T05:39:47.979896Z","shell.execute_reply.started":"2023-11-27T05:38:21.053770Z","shell.execute_reply":"2023-11-27T05:39:47.978402Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.1 which is incompatible.\ngoogle-cloud-pubsublite 1.8.3 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.0 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.0.1 requires kubernetes<27,>=8.0.0, but you have kubernetes 28.1.0 which is incompatible.\nopentelemetry-exporter-otlp 1.19.0 requires opentelemetry-exporter-otlp-proto-grpc==1.19.0, but you have opentelemetry-exporter-otlp-proto-grpc 1.21.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-exporter-otlp-proto-common==1.19.0, but you have opentelemetry-exporter-otlp-proto-common 1.21.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-proto==1.19.0, but you have opentelemetry-proto 1.21.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-sdk~=1.19.0, but you have opentelemetry-sdk 1.21.0 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.3 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\ntensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\n\nuser_secrets = UserSecretsClient()\nos.environ['HUGGINGFACEHUB_API_TOKEN'] = user_secrets.get_secret(\"HF_READ_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:39:53.059397Z","iopub.execute_input":"2023-11-27T05:39:53.059859Z","iopub.status.idle":"2023-11-27T05:39:53.279334Z","shell.execute_reply.started":"2023-11-27T05:39:53.059817Z","shell.execute_reply":"2023-11-27T05:39:53.278145Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import bs4\nfrom langchain import hub\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import HuggingFaceHub\nfrom langchain.chains import ConversationalRetrievalChain\nimport gradio as gr","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:39:54.763666Z","iopub.execute_input":"2023-11-27T05:39:54.764793Z","iopub.status.idle":"2023-11-27T05:40:02.698193Z","shell.execute_reply.started":"2023-11-27T05:39:54.764739Z","shell.execute_reply":"2023-11-27T05:40:02.697100Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"loader = WebBaseLoader(\n    web_path=('http://karpathy.github.io/2019/04/25/recipe/',),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(class_=('post-content', 'post-title', 'post-header'))\n    ),\n)\ndocs = loader.load()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:08.518727Z","iopub.execute_input":"2023-11-27T05:40:08.519472Z","iopub.status.idle":"2023-11-27T05:40:08.721268Z","shell.execute_reply.started":"2023-11-27T05:40:08.519432Z","shell.execute_reply":"2023-11-27T05:40:08.720109Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"len(docs)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:10.405356Z","iopub.execute_input":"2023-11-27T05:40:10.406453Z","iopub.status.idle":"2023-11-27T05:40:10.414897Z","shell.execute_reply.started":"2023-11-27T05:40:10.406414Z","shell.execute_reply":"2023-11-27T05:40:10.413600Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:10.748523Z","iopub.execute_input":"2023-11-27T05:40:10.749004Z","iopub.status.idle":"2023-11-27T05:40:10.761910Z","shell.execute_reply.started":"2023-11-27T05:40:10.748964Z","shell.execute_reply":"2023-11-27T05:40:10.760558Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(splits)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:12.442585Z","iopub.execute_input":"2023-11-27T05:40:12.443027Z","iopub.status.idle":"2023-11-27T05:40:12.451962Z","shell.execute_reply.started":"2023-11-27T05:40:12.442994Z","shell.execute_reply":"2023-11-27T05:40:12.450031Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"34"},"metadata":{}}]},{"cell_type":"code","source":"vectorstore = Chroma.from_documents(documents=splits, embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\nretriever = vectorstore.as_retriever()","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:17.596705Z","iopub.execute_input":"2023-11-27T05:40:17.597107Z","iopub.status.idle":"2023-11-27T05:40:52.747113Z","shell.execute_reply.started":"2023-11-27T05:40:17.597075Z","shell.execute_reply":"2023-11-27T05:40:52.745830Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ed951ea0674c2ca3d26e224f3f5aad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"354fe6bf58e54a828fe9f4a4b08536d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c29eeb52c64cdab3612c4cd12c3701"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b689e2bd93f741589d477543094459a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5599771ca70414089968578f77fe174"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88d7c1d0f4f44e15bc750f8c4b15daa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"209560f9b1804288bc931ebffa262f4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be50e98eb65a46a88dad44c80e89f095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9da3ed5ea6d0494a804f8e96ea1eb4f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c38830afd9a64062bda80168ff7c6c2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dc5a869855e45af8bbb34a30fc53508"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e6c6b2ffd38432b893e1a38c94a1d89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ea083c91954057874c92d853cb069d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ed130860fd64957af666700e6fc70e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8378154e507545699fe6a4e7d2682cb0"}},"metadata":{}}]},{"cell_type":"code","source":"prompt = hub.pull(\"rlm/rag-prompt\")\nllm = HuggingFaceHub(repo_id='HuggingFaceH4/zephyr-7b-beta', model_kwargs={\"temperature\": 0.5, \"max_length\": 4096})","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:52.749133Z","iopub.execute_input":"2023-11-27T05:40:52.749877Z","iopub.status.idle":"2023-11-27T05:40:53.482014Z","shell.execute_reply.started":"2023-11-27T05:40:52.749841Z","shell.execute_reply":"2023-11-27T05:40:53.480670Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n  warnings.warn(warning_message, FutureWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"llm","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:55.127543Z","iopub.execute_input":"2023-11-27T05:40:55.128912Z","iopub.status.idle":"2023-11-27T05:40:55.137676Z","shell.execute_reply.started":"2023-11-27T05:40:55.128863Z","shell.execute_reply":"2023-11-27T05:40:55.136186Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"HuggingFaceHub(client=InferenceAPI(api_url='https://api-inference.huggingface.co/pipeline/text-generation/HuggingFaceH4/zephyr-7b-beta', task='text-generation', options={'wait_for_model': True, 'use_gpu': False}), repo_id='HuggingFaceH4/zephyr-7b-beta', model_kwargs={'temperature': 0.5, 'max_length': 4096})"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.chains import ConversationalRetrievalChain\n\nqa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:56.165720Z","iopub.execute_input":"2023-11-27T05:40:56.166139Z","iopub.status.idle":"2023-11-27T05:40:56.173852Z","shell.execute_reply.started":"2023-11-27T05:40:56.166106Z","shell.execute_reply":"2023-11-27T05:40:56.172415Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"chat_history = []","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:40:57.888863Z","iopub.execute_input":"2023-11-27T05:40:57.889254Z","iopub.status.idle":"2023-11-27T05:40:57.893043Z","shell.execute_reply.started":"2023-11-27T05:40:57.889224Z","shell.execute_reply":"2023-11-27T05:40:57.892210Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"question = \"What is becoming one with the data?\"\nresult = qa({\"question\": question, \"chat_history\": chat_history})\nchat_history.append((question, result[\"answer\"]))\nprint(result[\"answer\"])","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:41:01.285930Z","iopub.execute_input":"2023-11-27T05:41:01.286537Z","iopub.status.idle":"2023-11-27T05:41:07.400476Z","shell.execute_reply.started":"2023-11-27T05:41:01.286503Z","shell.execute_reply":"2023-11-27T05:41:07.399105Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"465769ec76bf439186e5350d3c628b4a"}},"metadata":{}},{"name":"stdout","text":" Overfitting. When a model is so complex that it can memorize the training data, it is said to be overfitting. In this stage, the goal is to find a balance between complexity and generalization. This is achieved by regularization techniques that penalize complex models and encourage simpler ones. The optimal level of regularization depends on the problem and the dataset, and finding it requires iterating through different levels of regularization. The key is to ensure that the model can still generalize to unseen data, and not just memorize the training set. This is a critical stage in the model development process, as a model that overfits will not perform well on new, unseen data.\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_article(path):\n    loader = WebBaseLoader(\n        web_path=(path,),\n        bs_kwargs=dict(\n            parse_only=bs4.SoupStrainer(class_=('post-content', 'post-title', 'post-header'))\n        ),\n    )\n    docs = loader.load()\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n    splits = text_splitter.split_documents(docs)\n    vectorstore = Chroma.from_documents(documents=splits, embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n    retriever = vectorstore.as_retriever()\n    prompt = hub.pull(\"rlm/rag-prompt\")\n    llm = HuggingFaceHub(repo_id='HuggingFaceH4/zephyr-7b-beta', model_kwargs={\"temperature\": 0.5, \"max_length\": 4096})\n    global qa\n    qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)\n    return 'Ready!!!'","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:41:15.162752Z","iopub.execute_input":"2023-11-27T05:41:15.163195Z","iopub.status.idle":"2023-11-27T05:41:15.172700Z","shell.execute_reply.started":"2023-11-27T05:41:15.163163Z","shell.execute_reply":"2023-11-27T05:41:15.171478Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"load_article(path='http://karpathy.github.io/2019/04/25/recipe/')","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:41:18.647837Z","iopub.execute_input":"2023-11-27T05:41:18.648273Z","iopub.status.idle":"2023-11-27T05:41:35.681228Z","shell.execute_reply.started":"2023-11-27T05:41:18.648231Z","shell.execute_reply":"2023-11-27T05:41:35.679828Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"296d5f0bd41742f8bb3c8f456781ef64"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n  warnings.warn(warning_message, FutureWarning)\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'Ready!!!'"},"metadata":{}}]},{"cell_type":"code","source":"def add_text(history, text):\n    history = history + [(text, None)]\n    return history, ''\n\ndef bot(history):\n    response = infer(history[-1][0])\n    history[-1][1] = response['result']\n    return history\n\ndef infer(question):\n    query = question\n    result = qa({'query': query})\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:41:37.528066Z","iopub.execute_input":"2023-11-27T05:41:37.528848Z","iopub.status.idle":"2023-11-27T05:41:37.536772Z","shell.execute_reply.started":"2023-11-27T05:41:37.528801Z","shell.execute_reply":"2023-11-27T05:41:37.535523Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"with gr.Blocks() as demo:\n    with gr.Column():\n        with gr.Row():\n            article = gr.Textbox(label='Article Path', placeholder='Enter the path')\n            article_status = gr.Textbox(label='Status', placeholder='', interactive=False)\n            load = gr.Button('Load Article...')\n        \n        chatbot = gr.Chatbot([], elem_id='Chatbot')\n        question = gr.Textbox(label='Question', placeholder='Type your query')\n        submit_btn = gr.Button('Submit')\n    \n    load.click(load_article, inputs=[article], outputs=[article_status], queue=False)\n    question.submit(add_text, [chatbot, question], [chatbot, question]).then(bot, chatbot, chatbot)\n    submit_btn.click(add_text, [chatbot, question], [chatbot, question]).then(bot, chatbot, chatbot)\n    \ndemo.launch(share=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T05:43:45.370145Z","iopub.execute_input":"2023-11-27T05:43:45.370520Z","iopub.status.idle":"2023-11-27T05:43:45.663216Z","shell.execute_reply.started":"2023-11-27T05:43:45.370489Z","shell.execute_reply":"2023-11-27T05:43:45.661602Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgr\u001b[49m\u001b[38;5;241m.\u001b[39mBlocks() \u001b[38;5;28;01mas\u001b[39;00m demo:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mColumn():\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m gr\u001b[38;5;241m.\u001b[39mRow():\n","\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"],"ename":"NameError","evalue":"name 'gr' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}