{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvuBe81xiDixVUTe+KERfw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ag0Ns01N4GCr",
        "outputId": "3d2637e2-6438-4456-d30c-e6252bb2f137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-07 07:28:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-10-07 07:28:55 (46.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "8I7GYnM14Kvk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "sme-w76u4XGA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EXfFP3Z4cKA",
        "outputId": "e9b450f0-ba3a-4bac-de94-b86c21459ae1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(''.join(chars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "copWi3V04dOt",
        "outputId": "25efe43e-fdbf-4625-f98d-16b2eb460193"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int2char = dict(enumerate(chars))"
      ],
      "metadata": {
        "id": "SMlYNKFU4n7n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(int2char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZNL3rXF42Az",
        "outputId": "d0bebc72-80e4-47d9-c2a3-27a1484f6c38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char2int = {ch:ii for ii, ch in int2char.items()}\n",
        "print(char2int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sph5fJ2y43w9",
        "outputId": "b89ac5ee-ffaa-4e56-b98b-b3772587d76c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "metadata": {
        "id": "I5TcTg1e495K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRIvMPo95CFk",
        "outputId": "ad0321c9-7f88-4706-ae58-4f9a3f7947d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
              "       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
              "       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
              "       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n",
              "       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n",
              "       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39,\n",
              "       56, 43,  1, 39, 50, 50,  1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56,\n",
              "       39, 58, 46, 43, 56,  1, 58, 53,  1, 42, 47, 43,  1, 58, 46, 39, 52,\n",
              "        1, 58, 53,  1, 44, 39, 51, 47, 57, 46, 12,  0,  0, 13, 50, 50, 10,\n",
              "        0, 30, 43, 57, 53, 50, 60, 43, 42,  8,  1, 56, 43, 57, 53, 50, 60,\n",
              "       43, 42,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43,\n",
              "       52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63, 53, 59,  1, 49, 52, 53,\n",
              "       61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41, 47, 59, 57,  1, 47,\n",
              "       57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,  1, 58, 53,  1,\n",
              "       58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13, 50, 50, 10,\n",
              "        0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1, 49, 52,\n",
              "       53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47,\n",
              "       64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
              "       46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39,\n",
              "       60, 43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61,\n",
              "       52,  1, 54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60,\n",
              "       43, 56, 42, 47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1,\n",
              "       51, 53, 56, 43,  1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58,\n",
              "       11,  1, 50, 43, 58,  1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,\n",
              "        1, 39, 61, 39, 63,  6,  1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41,\n",
              "       53, 52, 42,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 27, 52, 43,  1,\n",
              "       61, 53, 56, 42,  6,  1, 45, 53, 53, 42,  1, 41, 47, 58, 47, 64, 43,\n",
              "       52, 57,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43,\n",
              "       52, 10,  0, 35, 43,  1, 39, 56, 43,  1, 39, 41, 41, 53, 59, 52, 58,\n",
              "       43, 42,  1, 54, 53, 53, 56])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "umDyAcGb5Dvn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seq = np.array([3, 5, 1])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NuN2yMm5W2U",
        "outputId": "cacbaa36-48a0-429e-e2b7-dc434367834b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "P8-w_oNm5fvR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches = get_batches(encoded, 8, 200)\n",
        "x, y = next(batches)"
      ],
      "metadata": {
        "id": "XAKrGNof6f4P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbCgcV3B6rJ6",
        "outputId": "119885f6-dd6b-4db2-a38d-620009809713"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8, 200), (8, 200))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eMhEUSMACMng",
        "outputId": "4620c537-d654-4d39-c3d5-21786c50fa97"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.2, lr=0.0001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: i for i, ch in self.int2char.items()}\n",
        "\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        out = self.dropout(r_output)\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "mizmlRnB6wGi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, epochs=10, batch_size=10, seq_length=200, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    model.train()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    val_idx = int(len(data) * (1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    model.to(device)\n",
        "\n",
        "    counter = 0\n",
        "    n_chars = len(model.chars)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        h = model.init_hidden(batch_size)\n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            h = tuple([each.data for each in h])\n",
        "            model.zero_grad()\n",
        "            output, h = model(inputs, h)\n",
        "\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            if counter % print_every == 0:\n",
        "                val_h = model.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                model.eval()\n",
        "\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    inputs, targets = x.to(device), y.to(device)\n",
        "                    output, val_h = model(inputs, val_h)\n",
        "\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                model.train()\n",
        "                print(f\"Epoch {e}/{epochs} Step {counter} Loss {loss.item():.4f} Val_Loss {np.mean(val_losses):.4f}\")"
      ],
      "metadata": {
        "id": "ESG58Mf18iy_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 512\n",
        "n_layers = 2\n",
        "\n",
        "model = CharRNN(chars, n_hidden, n_layers)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydRD_GkFAWHW",
        "outputId": "ee896463-d881-41c4-a0ed-6f064b5b4aaa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "seq_length = 500\n",
        "n_epochs = 50"
      ],
      "metadata": {
        "id": "xWiv1SwLAhmS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.005, print_every=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjL7lcLVArfC",
        "outputId": "939b0760-61b7-4786-d88a-3a0cacdfd7b6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50 Step 100 Loss 1.5235 Val_Loss 1.6882\n",
            "Epoch 13/50 Step 200 Loss 1.3877 Val_Loss 1.6277\n",
            "Epoch 19/50 Step 300 Loss 1.3190 Val_Loss 1.6180\n",
            "Epoch 26/50 Step 400 Loss 1.2261 Val_Loss 1.6287\n",
            "Epoch 33/50 Step 500 Loss 1.1580 Val_Loss 1.6429\n",
            "Epoch 39/50 Step 600 Loss 1.1088 Val_Loss 1.6714\n",
            "Epoch 46/50 Step 700 Loss 1.0502 Val_Loss 1.7373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, char, h=None, top_k=None):\n",
        "    x = np.array([[model.char2int[char]]])\n",
        "    x = one_hot_encode(x, len(model.chars))\n",
        "    inputs = torch.from_numpy(x)\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    h = tuple([each.data for each in h])\n",
        "    out, h = model(inputs, h)\n",
        "\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    p = p.cpu()\n",
        "\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(model.chars))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p/p.sum())\n",
        "    return model.int2char[char], h"
      ],
      "metadata": {
        "id": "fi62gIGOA4p7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, size, prime_str='The', top_k=None):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    chars = [ch for ch in prime_str]\n",
        "    h = model.init_hidden(1)\n",
        "    for ch in prime_str:\n",
        "        char, h = predict(model, ch, h, top_k=top_k)\n",
        "    chars.append(char)\n",
        "\n",
        "    for i in range(size):\n",
        "        char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "wGc8_NAnFeQy"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample(model, 5000, prime_str='The', top_k=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyJBKtcjGK8T",
        "outputId": "fd960c28-653c-4638-f6bb-6a20fb949305"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These wonderings, and heavens true,\n",
            "When you have long intend for his master's life,\n",
            "I am a gentlewoman of all another\n",
            "But one of trespories and do you began,\n",
            "But that I may make haste, take him the story.\n",
            "\n",
            "CRICIS:\n",
            "That is no oness, to give me from the rank\n",
            "And the best brater shousops that would have\n",
            "Forbiddenly a silent lipstreation\n",
            "On our revenge, and a greating defend\n",
            "That the commonder of a son and good.\n",
            "\n",
            "JULIET:\n",
            "If I do well, give, some awe the conselver,\n",
            "Thy fair stay sin as well to the course,\n",
            "To see how yet have leed upon his head.\n",
            "\n",
            "CLARENCE:\n",
            "And say you be a bride?\n",
            "\n",
            "KING RICHARD III:\n",
            "What is his bone of word?\n",
            "\n",
            "AUTOLYCUS:\n",
            "\n",
            "ANGELO:\n",
            "What! would I had the state and damnable?\n",
            "\n",
            "Third Citizen:\n",
            "No, good sir, go to.\n",
            "\n",
            "MENENIUS:\n",
            "Nuth up the breath.\n",
            "\n",
            "MENENIUS:\n",
            "I thank you betides.\n",
            "\n",
            "Clown:\n",
            "He shall be sended.\n",
            "\n",
            "CORIOLANUS:\n",
            "Worthy a man for since.\n",
            "\n",
            "LEONTES:\n",
            "Nay, but the soul of sick offercamest are foul to the\n",
            "person and the princes of his brow, she's spared, the\n",
            "deep in this deep strive, with a strew between\n",
            "A month the grave, we'll send to meen your suitor:\n",
            "I am none to be so, sir, a maid\n",
            "with me and the recompanion of the good,\n",
            "buy shows, his soul the letter, thus to stand and\n",
            "learn of my beauty is an executioner.\n",
            "\n",
            "POMPEY:\n",
            "Sir, I will crown'd\n",
            "With him a present cheek.\n",
            "\n",
            "MENENIUS:\n",
            "I am come to--\n",
            "\n",
            "SICINIUS:\n",
            "I know now, say you have been seen in a strange foul\n",
            "of all the beggar, with an unstanl'd witchrant.\n",
            "\n",
            "All:\n",
            "We shall bring in the seas of the envious pause. \n",
            "ISABELLA:\n",
            "As well as I deserver indeed spokens,\n",
            "I am a poor for mad thou wilt be set\n",
            "With me again and bear of lawful mark,\n",
            "As mock in that the rest of this see hands\n",
            "With themself and the higher doem a side\n",
            "Of steal and a true of this; alack the world,\n",
            "Which, was am letters on my life in him.\n",
            "\n",
            "JULIET:\n",
            "O God! what same hath been believed, and\n",
            "I cannot see, it is not service?\n",
            "\n",
            "Second Gentleman:\n",
            "And yet I cursed it in the people.\n",
            "\n",
            "COMINIUS:\n",
            "That's moved and say and hideing the rough the\n",
            "sun.\n",
            "\n",
            "SICINIUS:\n",
            "I that were neather here, being the fight\n",
            "wint of the poor of the wildows and\n",
            "the country, as the court-own steed should\n",
            "be all foul children to assemble which you\n",
            "have, all the water of the good for you\n",
            "should be some person with your battle, and the\n",
            "sea of you would have made her hour a practient.\n",
            "\n",
            "Clown:\n",
            "Ay, but that the royal prince, that says fetch.\n",
            "\n",
            "MENENIUS:\n",
            "The gods draw now.\n",
            "\n",
            "Shepherd:\n",
            "And when I must be sith'd as I am not a worthy\n",
            "Were it to seemed; and there will deserve into me\n",
            "An enemy them alleady, bold or steel'd\n",
            "As mines of honesty would be so beside\n",
            "With them to body in him. Yet stand all seat,\n",
            "Who speaks well speaking as I have a honourable.\n",
            "\n",
            "ROMEO:\n",
            "What case you so?\n",
            "\n",
            "CLARENCE:\n",
            "I thank thee, good Marshous.\n",
            "\n",
            "CORIOLANUS:\n",
            "Well, well, I will not denied you be as\n",
            "any tide the from my peal, which as you are.\n",
            "\n",
            "LUCIO:\n",
            "A man i' the madam, and the stilf and my boy;\n",
            "I have no perifue seath, from whele hath a\n",
            "granted of his eyes of the change\n",
            "on him shall bear with him. I am suff you\n",
            "are all about the garland. What, art thou,\n",
            "Came to the crown, which stay with him? for would\n",
            "have well approphess'd in thy self and stay too\n",
            "\n",
            "Richrding:\n",
            "An one feighle speak, as ill because they\n",
            "sooner word. Here comes his manners as you\n",
            "are out another. Come on thee, left a horse,\n",
            "please you and homine is this, by that answer's watch\n",
            "So sluck upon's.\n",
            "\n",
            "AUFIDIUS:\n",
            "O, that may say it is a present\n",
            "done out of your affection, but waters in\n",
            "the city's ears a posses.\n",
            "\n",
            "SICINIUS:\n",
            "He hear he will resolve my counsel.\n",
            "\n",
            "ABHORSON:\n",
            "Well, yet not the manner.\n",
            "\n",
            "MOPSA:\n",
            "I am common tent anon.\n",
            "\n",
            "Second Servingman:\n",
            "Where's the mouse, my lord;\n",
            "I had a shepherd's body, which the sun\n",
            "And bear the best.\n",
            "\n",
            "PAULINA:\n",
            "Not thou shalt know\n",
            "That whitees of me to the careful week,\n",
            "And this thousand that she in his bosom there\n",
            "Was ne'er shall passed to her approbeties\n",
            "And make a party such a bold of them.\n",
            "What shall bring foul at once a word, though would have\n",
            "said, she--perchive, bound to have an old son,\n",
            "At it so heap of death?\n",
            "\n",
            "PARIS:\n",
            "Why, being a steel seemence,\n",
            "You may not be as most be setsing\n",
            "To see his sensence. What, are your greater?\n",
            "\n",
            "JULIET:\n",
            "O thou dost sweet as thire in a most sweet,\n",
            "I mean to come but framed and had the sentence\n",
            "That wash he both a sail.\n",
            "\n",
            "PAULINA:\n",
            "I think hope,\n",
            "We must suppose it still before you caught.\n",
            "\n",
            "POMPEY:\n",
            "If you have seen thee, stand us with your hands of\n",
            "wineish contrady, as have all\n",
            "them are to prove. Your good cry 'Chees and\n",
            "To be to bring our brather's senses, whom they should\n",
            "be so much in your hand: and so you shall\n",
            "be and to say it is the selfon a cause foul touch.\n",
            "\n",
            "First Senator:\n",
            "A bord, yet still have been a prison. Your bastle,\n",
            "Where he would serve my bands. You have no poison,\n",
            "bay,\n",
            "With the court in the bowles of the stanging natus\n",
            "\n",
            "ERCIIUS:\n",
            "And see, sir, happy there no lander.\n",
            "\n",
            "MENENIUS:\n",
            "He'rs a weapon, sir, as the powerful bend\n",
            "And all that thou shouldst spake it, as it is;\n",
            "The name of England and myself are brief,\n",
            "With all the heavens deny. This is the milknish,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mgr4InXtGawD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}